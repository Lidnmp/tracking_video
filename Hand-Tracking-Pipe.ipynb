{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from IPython.display import clear_output, Image, display\n",
    "from PIL import Image as PILImage\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_jupyter(frame, processed=None):\n",
    "    \"\"\"Display frame(s) in Jupyter notebook\"\"\"\n",
    "    \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display side by side if we have two frames\n",
    "    if processed is not None:\n",
    "        \n",
    "        processed_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        combined = np.hstack((rgb_frame, processed_rgb))\n",
    "        pil_img = PILImage.fromarray(combined)\n",
    "    else:\n",
    "        pil_img = PILImage.fromarray(rgb_frame)\n",
    "    \n",
    "    # Create binary stream\n",
    "    bio = io.BytesIO()\n",
    "    pil_img.save(bio, format='PNG')\n",
    "    \n",
    "    display(Image(data=bio.getvalue()))\n",
    "    clear_output(wait=True)  # Clear previous frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cv2(frame, processed=None):\n",
    "    \"\"\"Display frame(s) using OpenCV window\"\"\"\n",
    "    if processed is not None:\n",
    "        # Stack frames horizontally\n",
    "        combined = np.hstack((frame, processed))\n",
    "        cv2.imshow('Video Feed', combined)\n",
    "    else:\n",
    "        cv2.imshow('Video Feed', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_initial_point(frame):\n",
    "    \"\"\"Process frame to detect pointing gesture and get index fingertip\"\"\"\n",
    "    \n",
    "    # Initialize gesture recognizer of mediapipe\n",
    "    base_options = python.BaseOptions(model_asset_path='models/gesture_recognizer.task')\n",
    "    options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "    gesture_recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "    \n",
    "    # Detect gestures from the frame\n",
    "    gesture_result = gesture_recognizer.recognize(mp_image)\n",
    "    \n",
    "    index_tip = None\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "    \n",
    "    gesture_result = gesture_recognizer.recognize(mp_image)\n",
    "    \n",
    "    # The landmark 8 is the tip of the index finger based on the mediapipe hand landmarks\n",
    "    # Available at https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker#get_started\n",
    "    if gesture_result.gestures and gesture_result.hand_landmarks:\n",
    "        top_gesture = gesture_result.gestures[0]\n",
    "        if top_gesture[0].category_name == \"Pointing_Up\":\n",
    "            hand_landmarks = gesture_result.hand_landmarks[0]\n",
    "            index_tip = (\n",
    "                int(hand_landmarks[8].x * frame.shape[1]),\n",
    "                int(hand_landmarks[8].y * frame.shape[0])\n",
    "            )\n",
    "            gesture_recognizer.close()\n",
    "            return index_tip\n",
    "    \n",
    "    gesture_recognizer.close()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video path\n",
    "video_path = \"hand_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video file\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "# State variables\n",
    "initial_point = None\n",
    "tracking_started = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        \n",
    "        output_frame = frame.copy()\n",
    "        \n",
    "        if not tracking_started:\n",
    "            # Try to detect initial pointing gesture\n",
    "            initial_point = detect_initial_point(frame)\n",
    "            if initial_point is not None:\n",
    "                tracking_started = True\n",
    "                print(\"Initial point detected! Starting tracking...\")\n",
    "                # Here we would initialize any tracking-specific variables or algorithms\n",
    "        \n",
    "        if tracking_started:\n",
    "            cv2.circle(output_frame, initial_point, 5, (0, 255, 0), -1)\n",
    "            cv2.putText(output_frame, \"Tracking Active\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Here we would implement our own tracking logic\n",
    "        else:\n",
    "            cv2.putText(output_frame, \"Waiting for pointing gesture...\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Display the frame in jupyter notebook or OpenCV window\n",
    "        display_jupyter(frame, output_frame)\n",
    "        \n",
    "        # Add a small delay to control display speed\n",
    "        cv2.waitKey(int(1000/fps))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "finally:\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738858581.011594 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.012665 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.016690 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1738858581.034047 88864757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.049848 88864758 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.051534 88864761 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.051573 88864761 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/farouq/Documents/School/UIB/CV - Image and Video Analysis/exercises/tracking_video/.venv/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "I0000 00:00:1738858581.277597 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.277816 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.278552 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1738858581.286034 88864815 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.291449 88864818 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.291817 88864820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.291853 88864820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1738858581.374794 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.374997 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.375718 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1738858581.380585 88864829 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.385771 88864835 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.386149 88864830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.386184 88864830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1738858581.470756 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.470974 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.471802 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1738858581.477410 88864839 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.483113 88864839 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.483613 88864844 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.483649 88864844 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1738858581.565882 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.566101 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.566934 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1738858581.572711 88864847 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.577136 88864846 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.577495 88864846 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.577529 88864846 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-02-06 17:16:21.660 Python[55592:88864218] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-06 17:16:21.660 Python[55592:88864218] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "I0000 00:00:1738858581.682827 88864218 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738858581.683056 88864218 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1738858581.683900 88864218 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1738858581.689835 88864867 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.695012 88864872 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.695369 88864867 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738858581.695404 88864867 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial point detected! Starting tracking...\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    tracker = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        \n",
    "        output_frame = frame.copy()\n",
    "        if not tracking_started:\n",
    "            \n",
    "            initial_point = detect_initial_point(frame)\n",
    "            if initial_point is not None:\n",
    "                tracking_started = True\n",
    "                print(\"Initial point detected! Starting tracking...\")\n",
    "                \n",
    "                # Initialize tracker\n",
    "                tracker = cv2.TrackerCSRT_create()  # or cv2.TrackerKCF_create()\n",
    "                \n",
    "                # Create bounding box around initial point of 50x50 pixels\n",
    "                # The goal of the bounding box is to create an area of interest for the tracker\n",
    "                box_size = 50\n",
    "                bbox = (\n",
    "                    initial_point[0] - box_size//2,\n",
    "                    initial_point[1] - box_size//2,\n",
    "                    box_size,\n",
    "                    box_size\n",
    "                )\n",
    "                tracker.init(frame, bbox)\n",
    "                current_point = initial_point\n",
    "        \n",
    "        if tracking_started:\n",
    "            # Update tracker\n",
    "            success, bbox = tracker.update(frame)\n",
    "            \n",
    "            if success:\n",
    "                # Get center point of bounding box for plotting\n",
    "                current_point = (\n",
    "                    int(bbox[0] + bbox[2]//2),\n",
    "                    int(bbox[1] + bbox[3]//2)\n",
    "                )\n",
    "                \n",
    "                cv2.circle(output_frame, current_point, 5, (0, 255, 0), -1)\n",
    "                cv2.rectangle(output_frame, \n",
    "                            (int(bbox[0]), int(bbox[1])), \n",
    "                            (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])),\n",
    "                            (255, 0, 0), 2)\n",
    "                cv2.putText(output_frame, \"Tracking Active\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(output_frame, \"Tracking Lost\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                tracking_started = False\n",
    "        else:\n",
    "            cv2.putText(output_frame, \"Waiting for pointing gesture...\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Display the frame in jupyter notebook or OpenCV window\n",
    "        display_cv2(frame, output_frame)\n",
    "        \n",
    "        # Add a small delay to control display speed\n",
    "        cv2.waitKey(int(1000/fps))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "finally:\n",
    "    cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
