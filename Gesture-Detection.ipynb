{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as PILImage\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_video(video_path):\n",
    "    \"\"\"Initialize the video and return the video capture object\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open video file\")\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"hand_tracking.mp4\"\n",
    "\n",
    "cap = initialize_video(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # If video ends, loop back to start\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        continue\n",
    "        \n",
    "    cv2.imshow('Original', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame, processed=None):\n",
    "    \"\"\"Display frame(s) in Jupyter notebook\"\"\"\n",
    "    \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display side by side if we have two frames\n",
    "    if processed is not None:\n",
    "        processed_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        combined = np.hstack((rgb_frame, processed_rgb))\n",
    "        pil_img = PILImage.fromarray(combined)\n",
    "    else:\n",
    "        pil_img = PILImage.fromarray(rgb_frame)\n",
    "    \n",
    "    # Create binary stream\n",
    "    bio = io.BytesIO()\n",
    "    pil_img.save(bio, format='PNG')\n",
    "    \n",
    "    # Display using IPython\n",
    "    display(Image(data=bio.getvalue()))\n",
    "    clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess the frame for hand detection\"\"\"\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(hsv, (5, 5), 0)\n",
    "    \n",
    "    # Define skin color range in HSV (not perfect. Capturing range of skin colours is challenging)\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    \n",
    "    skin_mask = cv2.inRange(blurred, lower_skin, upper_skin)\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    skin_mask = cv2.erode(skin_mask, kernel, iterations=2)\n",
    "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
    "    \n",
    "    skin = cv2.bitwise_and(frame, frame, mask=skin_mask)\n",
    "    \n",
    "    return skin, skin_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hand(frame, skin_mask):\n",
    "    \"\"\"Detect hand contour and fingertips\"\"\"\n",
    "    \n",
    "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If no contours found, return original frame\n",
    "    if not contours:\n",
    "        return frame, None, None\n",
    "    \n",
    "    # Get the largest contour (assumed to be the hand)\n",
    "    # This assumption is false. It doesn't work well and generalize. \n",
    "    # There are other areas bigger than the hands in the video\n",
    "    hand_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    hull = cv2.convexHull(hand_contour, returnPoints=False)\n",
    "    defects = cv2.convexityDefects(hand_contour, hull)\n",
    "    \n",
    "    output = frame.copy()\n",
    "    \n",
    "    cv2.drawContours(output, [hand_contour], -1, (0, 255, 0), 2)\n",
    "    \n",
    "    fingertips = []\n",
    "    if defects is not None:\n",
    "        for i in range(defects.shape[0]):\n",
    "            s, e, f, d = defects[i, 0]\n",
    "            start = tuple(hand_contour[s][0])\n",
    "            end = tuple(hand_contour[e][0])\n",
    "            far = tuple(hand_contour[f][0])\n",
    "            \n",
    "            a = np.sqrt((end[0] - start[0])**2 + (end[1] - start[1])**2)\n",
    "            b = np.sqrt((far[0] - start[0])**2 + (far[1] - start[1])**2)\n",
    "            c = np.sqrt((end[0] - far[0])**2 + (end[1] - far[1])**2)\n",
    "            angle = np.arccos((b**2 + c**2 - a**2)/(2*b*c)) * 57\n",
    "            \n",
    "            if angle <= 90:\n",
    "                fingertips.append(end)\n",
    "                cv2.circle(output, end, 5, (0, 0, 255), -1)\n",
    "    \n",
    "    return output, hand_contour, fingertips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "video_path = \"hand_tracking.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video file\")\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        \n",
    "        skin, skin_mask = preprocess_frame(frame)\n",
    "        \n",
    "        # Plot skin and skin_mask\n",
    "        cv2.imshow('Skin', skin)\n",
    "        cv2.imshow('Skin Mask', skin_mask)\n",
    "        \n",
    "        processed_frame, hand_contour, fingertips = detect_hand(frame, skin_mask)\n",
    "        \n",
    "        display_frame(frame, processed_frame)\n",
    "        \n",
    "        cv2.waitKey(int(1000/fps))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "finally:\n",
    "    cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
