{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import clear_output, Image, display\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from src.utils import (\n",
    "    display_jupyter,\n",
    "    display_cv2,\n",
    "    setup_gesture_recognizer,\n",
    "    detect_initial_point,\n",
    "    detect_closed_fist,\n",
    "    setup_text_detector,\n",
    "    detect_text_from_canvas,\n",
    "    draw_recognized_text,\n",
    ")\n",
    "\n",
    "from src.kalman_filter import KalmanFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "This section defines the video sources and configures the parameters used in the tracking setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the genAI recognizer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the GEMINI_API_KEY\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set\")\n",
    "\n",
    "text_detector = setup_text_detector(gemini_api_key)\n",
    "\n",
    "### Get the video source\n",
    "\n",
    "# Video path\n",
    "video_path = \"hand_tracking.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video file\")\n",
    "\n",
    "# Frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Canvas to draw the gestures\n",
    "canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "### State variables\n",
    "initial_point = None\n",
    "tracking_started = False\n",
    "points = []\n",
    "recognized_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "In this section, we perform the tracking task based on predefined algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking with CSRT Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gesture_recongizer = setup_gesture_recognizer()\n",
    "    tracker = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        output_frame = frame.copy()\n",
    "        \n",
    "        if not tracking_started:\n",
    "            # Detect the initial point using the MediaPipe library\n",
    "            initial_point = detect_initial_point(frame, gesture_recongizer)\n",
    "            \n",
    "            if initial_point is not None:\n",
    "                tracking_started = True\n",
    "                print(\"Initial point detected! Starting tracking...\")\n",
    "                \n",
    "                # Initialize tracker\n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "                \n",
    "                # Create bounding box around initial point of 50x50 pixels\n",
    "                box_size = 50\n",
    "                bbox = (\n",
    "                    initial_point[0] - box_size//2,\n",
    "                    initial_point[1] - box_size//2,\n",
    "                    box_size,\n",
    "                    box_size\n",
    "                )\n",
    "                tracker.init(frame, bbox)\n",
    "                current_point = initial_point\n",
    "                points = [initial_point]\n",
    "        \n",
    "        if tracking_started:\n",
    "            # Update tracker to get new bounding box\n",
    "            success, bbox = tracker.update(frame)\n",
    "            \n",
    "            # If tracking is successful, get the center point of the bounding box\n",
    "            if success:\n",
    "                current_point = (\n",
    "                    int(bbox[0] + bbox[2]//2),\n",
    "                    int(bbox[1] + bbox[3]//2)\n",
    "                )\n",
    "                \n",
    "                points.append(current_point)\n",
    "                \n",
    "                # Draw line on canvas\n",
    "                if len(points) > 1:\n",
    "                    cv2.line(canvas, points[-2], points[-1], (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                \n",
    "                # Combine canvas with output frame\n",
    "                output_frame = cv2.addWeighted(output_frame, 1.0, canvas, 1.0, 0)\n",
    "                \n",
    "                cv2.circle(output_frame, current_point, 5, (0, 255, 0), -1)\n",
    "                cv2.rectangle(output_frame, \n",
    "                            (int(bbox[0]), int(bbox[1])), \n",
    "                            (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])),\n",
    "                            (255, 0, 0), 2)\n",
    "                cv2.putText(output_frame, \"Tracking Active\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                if detect_closed_fist(frame, gesture_recongizer):\n",
    "                    tracking_started = False\n",
    "                    print(\"Closed fist detected - ending tracking\")\n",
    "                    \n",
    "                    # Perform OCR on the canvas\n",
    "                    if len(points) > 1:  # Only if something was drawn\n",
    "                        text = detect_text_from_canvas(canvas, text_detector)\n",
    "                \n",
    "                        recognized_text += f\"{text} \"\n",
    "                        print(f\"Recognized text: {recognized_text}\")\n",
    "                        \n",
    "                        canvas = np.zeros_like(frame)\n",
    "                    \n",
    "            else:\n",
    "                cv2.putText(output_frame, \"Tracking Lost\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Reset tracking variables\n",
    "                tracking_started = False\n",
    "                points = []\n",
    "                canvas = np.zeros_like(frame)\n",
    "        else:\n",
    "            # Combine existing canvas with output frame\n",
    "            output_frame = cv2.addWeighted(output_frame, 1.0, canvas, 0.5, 0)\n",
    "            cv2.putText(output_frame, \"Waiting for pointing gesture...\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Display the last recognized text as subtitle if it exists\n",
    "        if recognized_text:\n",
    "           draw_recognized_text(output_frame, recognized_text)\n",
    "        \n",
    "        display_cv2(frame, output_frame)\n",
    "        cv2.waitKey(int(1000/fps))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "finally:\n",
    "    gesture_recongizer.close()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking with Kalman Filters\n",
    "\n",
    "Tutorial Guide: https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking using Lucas-Kanade Optical Flow and Kalman Filter\n",
    "\n",
    "try:\n",
    "    gesture_recognizer = setup_gesture_recognizer()\n",
    "    tracker = None\n",
    "    old_frame = None\n",
    "    old_gray = None\n",
    "    \n",
    "    # Luka-Kanade parameters\n",
    "    lk_params = dict(winSize=(15, 15),\n",
    "                    maxLevel=2,\n",
    "                    criteria=(cv2.TERM_CRITERIA_EPS |\n",
    "                            cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    # Define the Kalman Filter\n",
    "    kf = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        output_frame = frame.copy()\n",
    "        \n",
    "        if not tracking_started:\n",
    "            initial_point = detect_initial_point(frame, gesture_recognizer)\n",
    "            \n",
    "            if initial_point is not None:\n",
    "                tracking_started = True\n",
    "                print(\"Initial point detected! Starting tracking...\")\n",
    "                \n",
    "                current_point = initial_point\n",
    "                points = [initial_point]\n",
    "                \n",
    "                p0 = np.array([[current_point[0], current_point[1]]], dtype=np.float32).reshape(-1, 1, 2)\n",
    "            \n",
    "            old_frame = frame\n",
    "            old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if tracking_started:\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            (kalman_pred_x, kalman_pred_y) = kf.predict()\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "            \n",
    "            if st[0][0] == 1:\n",
    "                measurement = p1.reshape(-1, 2)\n",
    "                x_meas, y_meas = measurement[0]\n",
    "                \n",
    "                (kalman_updated_x, kalman_updated_y) = kf.update([[x_meas], [y_meas]])\n",
    "                \n",
    "                points.append((int(kalman_updated_x), int(kalman_updated_y)))\n",
    "                \n",
    "                if len(points) > 1:\n",
    "                    cv2.line(canvas, points[-2], points[-1], (0, 0, 255), 3)\n",
    "                \n",
    "                output_frame = cv2.addWeighted(output_frame, 1.0, canvas, 1.0, 0)\n",
    "                \n",
    "                # Draw tracking visualization\n",
    "                cv2.circle(output_frame, (int(kalman_pred_x), int(kalman_pred_y)), 5, (0, 255, 255), -1)\n",
    "                cv2.circle(output_frame, (int(kalman_updated_x), int(kalman_updated_y)), 5, (0, 255, 0), -1)\n",
    "                cv2.rectangle(output_frame, \n",
    "                            (int(kalman_updated_x - 15), int(kalman_updated_y - 15)), \n",
    "                            (int(kalman_updated_x + 15), int(kalman_updated_y + 15)),\n",
    "                            (255, 0, 0), 2)\n",
    "                cv2.putText(output_frame, \"Tracking Active\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                # Check for closed fist gesture\n",
    "                if detect_closed_fist(frame, gesture_recognizer):\n",
    "                    print(\"Closed fist detected - ending tracking\")\n",
    "                    detected_text = detect_text_from_canvas(canvas, text_detector)\n",
    "                    if detected_text:\n",
    "                        recognized_text += f\"{detected_text} \"\n",
    "                    \n",
    "                    tracking_started = False\n",
    "                    points = []\n",
    "                    canvas = np.zeros_like(frame)\n",
    "                    continue\n",
    "                \n",
    "                p0 = p1.reshape(-1, 1, 2)\n",
    "                old_frame = frame\n",
    "                old_gray = frame_gray\n",
    "                \n",
    "            else:\n",
    "                p0 = np.array([[kalman_pred_x, kalman_pred_y]], dtype=np.float32).reshape(-1, 1, 2)\n",
    "                output_frame = cv2.addWeighted(output_frame, 1.0, canvas, 1.0, 0)\n",
    "                \n",
    "                cv2.circle(output_frame, (int(kalman_pred_x), int(kalman_pred_y)), 5, (0, 255, 255), -1)\n",
    "                cv2.putText(output_frame, \"Tracking Lost - Kalman Guesses only\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "                tracking_started = False\n",
    "                points = []\n",
    "                canvas = np.zeros_like(frame)\n",
    "        else:\n",
    "            output_frame = cv2.addWeighted(output_frame, 1.0, canvas, 1.0, 0)\n",
    "            cv2.putText(output_frame, \"Waiting for pointing gesture...\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw recognized text as subtitle\n",
    "        if recognized_text:\n",
    "            draw_recognized_text(output_frame, recognized_text)\n",
    "        \n",
    "        display_cv2(frame, output_frame)\n",
    "        cv2.waitKey(int(1000/fps))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "finally:\n",
    "    gesture_recognizer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
